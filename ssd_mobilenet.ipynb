{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Navu45/object-detection/blob/master/ssd_mobilenet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFuBfz5xiZnE"
      },
      "source": [
        "Copyright (c) Microsoft Corporation. All rights reserved.  \n",
        "Licensed under the MIT License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8Mqw63jiZnH"
      },
      "source": [
        "# Mobilenet v2 Quantization with ONNX Runtime on CPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_zDu7Y3iZnH"
      },
      "source": [
        "In this tutorial, we will load a mobilenet v2 model pretrained with [PyTorch](https://pytorch.org/), export the model to ONNX, and quantize then run with ONNXRuntime."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_37DthjiZnI"
      },
      "source": [
        "## 0. Prerequisites ##\n",
        "\n",
        "If you have Jupyter Notebook, you can run this notebook directly with it. You may need to install or upgrade [PyTorch](https://pytorch.org/), [OnnxRuntime](https://microsoft.github.io/onnxruntime/), and other required packages.\n",
        "\n",
        "Otherwise, you can setup a new environment. First, install [Anaconda](https://www.anaconda.com/distribution/). Then open an AnaConda prompt window and run the following commands:\n",
        "\n",
        "```console\n",
        "conda create -n cpu_env python=3.8\n",
        "conda activate cpu_env\n",
        "conda install jupyter\n",
        "jupyter notebook\n",
        "```\n",
        "The last command will launch Jupyter Notebook and we can open this notebook in browser to continue."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BH1gWIzMiZnI"
      },
      "source": [
        "### 0.1 Install packages\n",
        "Let's install the necessary packages to start the tutorial. We will install PyTorch 1.8, OnnxRuntime 1.8, latest ONNX and pillow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "scrolled": true,
        "id": "qE_npN7fiZnI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e600cfbd-011f-4100-bea0-0a235d365186"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: Could not find a version that satisfies the requirement torch==1.8.0 (from versions: 1.11.0, 1.11.0+cpu, 1.11.0+cu113, 1.11.0+cu115, 1.12.0, 1.12.0+cpu, 1.12.0+cu113, 1.12.0+cu116, 1.12.1, 1.12.1+cpu, 1.12.1+cu113, 1.12.1+cu116, 1.13.0, 1.13.0+cpu, 1.13.0+cu116, 1.13.0+cu117, 1.13.1, 1.13.1+cpu, 1.13.1+cu116, 1.13.1+cu117)\n",
            "ERROR: No matching distribution found for torch==1.8.0\n",
            "ERROR: Could not find a version that satisfies the requirement onnxruntime==1.8.0 (from versions: 1.12.0, 1.12.1, 1.13.1)\n",
            "ERROR: No matching distribution found for onnxruntime==1.8.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: onnx in c:\\users\\alexey\\anaconda3\\envs\\deep-learning-mptu\\lib\\site-packages (1.13.0)\n",
            "Requirement already satisfied: numpy>=1.16.6 in c:\\users\\alexey\\anaconda3\\envs\\deep-learning-mptu\\lib\\site-packages (from onnx) (1.23.3)\n",
            "Requirement already satisfied: protobuf<4,>=3.20.2 in c:\\users\\alexey\\anaconda3\\envs\\deep-learning-mptu\\lib\\site-packages (from onnx) (3.20.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.2.1 in c:\\users\\alexey\\anaconda3\\envs\\deep-learning-mptu\\lib\\site-packages (from onnx) (4.4.0)\n",
            "Requirement already satisfied: pillow in c:\\users\\alexey\\anaconda3\\envs\\deep-learning-mptu\\lib\\site-packages (9.4.0)\n"
          ]
        }
      ],
      "source": [
        "# Install or upgrade PyTorch 1.8.0 and OnnxRuntime 1.8 for CPU-only.\n",
        "import sys\n",
        "!{sys.executable} -m pip install --upgrade torch==1.8.0 torchvision==0.9.0 torchaudio===0.8.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!{sys.executable} -m pip install --upgrade onnxruntime==1.8.0\n",
        "!{sys.executable} -m pip install --upgrade onnx\n",
        "!{sys.executable} -m pip install --upgrade pillow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4OTHSw91iZnJ"
      },
      "source": [
        "# 1 Download pretrained model and export to ONNX"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7EX3ChiiZnK"
      },
      "source": [
        "In this step, we load a pretrained mobilenet v2 model, and export it to ONNX."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAe69oJeiZnK"
      },
      "source": [
        "### 1.1 Load the pretrained model\n",
        "Use torchvision provides API to load mobilenet_v2 model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ESmhqdvMiZnK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5763242d-0e26-46c4-f39e-d6b71f84daf6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\Alexey\\anaconda3\\envs\\deep-learning-mptu\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "C:\\Users\\Alexey\\anaconda3\\envs\\deep-learning-mptu\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=SSDLite320_MobileNet_V3_Large_Weights.COCO_V1`. You can also use `weights=SSDLite320_MobileNet_V3_Large_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "from torchvision import models, datasets, transforms as T\n",
        "ssdlite_mobilenet_v3 = models.detection.ssdlite320_mobilenet_v3_large(pretrained=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfyO9xzriZnL"
      },
      "source": [
        "### 1.2 Export the model to ONNX\n",
        "Pytorch onnx export API to export the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "xWtY7AtSiZnL"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "image_height = 320\n",
        "image_width = 320\n",
        "x = torch.randn(1, 3, image_height, image_width, requires_grad=True)\n",
        "ssdlite_mobilenet_v3.eval()\n",
        "with torch.no_grad():\n",
        "  torch_out = ssdlite_mobilenet_v3(x)\n",
        "  # Export the model\n",
        "  torch.onnx.export(ssdlite_mobilenet_v3,              # model being run\n",
        "                    x,                         # model input (or a tuple for multiple inputs)\n",
        "                    \"ssdlite_mobilenet_v3.onnx\", # where to save the model (can be a file or file-like object)\n",
        "                    export_params=True,        # store the trained parameter weights inside the model file\n",
        "                    opset_version=12,          # the ONNX version to export the model to\n",
        "                    do_constant_folding=True,  # whether to execute constant folding for optimization\n",
        "                    input_names = ['input'],   # the model's input names\n",
        "                    output_names = ['output']) # the model's output names\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJez42AwiZnL"
      },
      "source": [
        "### 1.3 Sample Execution with ONNXRuntime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAoF7XQCiZnM"
      },
      "source": [
        "Run an sample with the full precision ONNX model. Firstly, implement the preprocess."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "vtRH-kvdiZnM"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "import onnxruntime\n",
        "import torch\n",
        "\n",
        "def preprocess_image(image_path, height, width, channels=3):\n",
        "    image = Image.open(image_path)\n",
        "    image = image.resize((width, height), Image.ANTIALIAS)\n",
        "    image_data = np.asarray(image).astype(np.float32)\n",
        "    image_data = image_data.transpose([2, 0, 1]) # transpose to CHW\n",
        "    mean = np.array([0.079, 0.05, 0]) + 0.406\n",
        "    std = np.array([0.005, 0, 0.001]) + 0.224\n",
        "    for channel in range(image_data.shape[0]):\n",
        "        image_data[channel, :, :] = (image_data[channel, :, :] / 255 - mean[channel]) / std[channel]\n",
        "    image_data = np.expand_dims(image_data, 0)\n",
        "    return image_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Dnqgab8iZnM"
      },
      "source": [
        "#### Download the imagenet labels and load it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "rhqQCeE5iZnM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c47302d3-1d5e-4b54-f416-0dd559c7e7dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100 10472  100 10472    0     0  35339      0 --:--:-- --:--:-- --:--:-- 35378\n"
          ]
        }
      ],
      "source": [
        "# Download ImageNet labels\n",
        "!curl -o imagenet_classes.txt https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\n",
        "\n",
        "# Read the categories\n",
        "with open(\"imagenet_classes.txt\", \"r\") as f:\n",
        "    categories = [s.strip() for s in f.readlines()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqAPYsFhiZnM"
      },
      "source": [
        "#### Run the example with ONNXRuntime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "nt-7KyWHiZnM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13286b1d-4f87-4b55-f25b-61d83d3f07c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "banded gecko 0.03943445\n",
            "jellyfish 0.03943445\n",
            "wombat 0.03943445\n",
            "oxcart 0.03943445\n",
            "leatherback turtle 0.03943445\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\Alexey\\AppData\\Local\\Temp\\ipykernel_7900\\2249352223.py:8: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n",
            "  image = image.resize((width, height), Image.ANTIALIAS)\n"
          ]
        }
      ],
      "source": [
        "session_fp32 = onnxruntime.InferenceSession(\"ssdlite_mobilenet_v3.onnx\")\n",
        "\n",
        "def softmax(x):\n",
        "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return e_x / e_x.sum()\n",
        "\n",
        "def run_sample(session, image_file, categories):\n",
        "    output = session.run([], {'input':preprocess_image(image_file, image_height, image_width)})[0]\n",
        "    output = output.flatten()\n",
        "    output = softmax(output) # this is optional\n",
        "    top5_catid = np.argsort(-output)[:5]\n",
        "    for catid in top5_catid:\n",
        "        print(categories[catid], output[catid])\n",
        "\n",
        "run_sample(session_fp32, 'cat.jpg', categories)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vx-sd79BiZnN"
      },
      "source": [
        "# 2 Quantize the model with ONNXRuntime \n",
        "In this step, we load the full precison model, and quantize it with ONNXRuntime quantization tool. And show the model size comparison between full precision and quantized model. Finally, we run the same sample with the quantized model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bI-9dMd5iZnN"
      },
      "source": [
        "## 2.1 Implement a CalibrationDataReader\n",
        "CalibrationDataReader takes in calibration data and generates input for the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "hXOHlsAviZnN"
      },
      "outputs": [],
      "source": [
        "from onnxruntime.quantization import quantize_static, CalibrationDataReader, QuantType\n",
        "import os\n",
        "\n",
        "def preprocess_func(images_folder, height, width, size_limit=0):\n",
        "    image_names = os.listdir(images_folder)\n",
        "    if size_limit > 0 and len(image_names) >= size_limit:\n",
        "        batch_filenames = [image_names[i] for i in range(size_limit)]\n",
        "    else:\n",
        "        batch_filenames = image_names\n",
        "    unconcatenated_batch_data = []\n",
        "\n",
        "    for image_name in batch_filenames:\n",
        "        image_filepath = images_folder + '/' + image_name\n",
        "        image_data = preprocess_image(image_filepath, height, width)\n",
        "        unconcatenated_batch_data.append(image_data)\n",
        "    batch_data = np.concatenate(np.expand_dims(unconcatenated_batch_data, axis=0), axis=0)\n",
        "    return batch_data\n",
        "\n",
        "\n",
        "class MobilenetDataReader(CalibrationDataReader):\n",
        "    def __init__(self, calibration_image_folder):\n",
        "        self.image_folder = calibration_image_folder\n",
        "        self.preprocess_flag = True\n",
        "        self.enum_data_dicts = []\n",
        "        self.datasize = 0\n",
        "\n",
        "    def get_next(self):\n",
        "        if self.preprocess_flag:\n",
        "            self.preprocess_flag = False\n",
        "            nhwc_data_list = preprocess_func(self.image_folder, image_height, image_width, size_limit=0)\n",
        "            self.datasize = len(nhwc_data_list)\n",
        "            self.enum_data_dicts = iter([{'input': nhwc_data} for nhwc_data in nhwc_data_list])\n",
        "        return next(self.enum_data_dicts, None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ne-IOLwKiZnN"
      },
      "source": [
        "## 2.2 Quantize the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNRcr88eiZnN"
      },
      "source": [
        "As we can not upload full calibration data set for copy right issue, we only demonstrate with some example images. You need to use your own calibration data set in practice."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m wget https://github.com/microsoft/onnxruntime-inference-examples/tree/main/quantization/notebooks/imagenet_v2/calibration_imagenet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxe6OP9IkEcr",
        "outputId": "5f29ab0e-3859-48ee-ce48-82fbd964a7a3"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"C:\\Users\\Alexey\\anaconda3\\envs\\deep-learning-mptu\\lib\\runpy.py\", line 196, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"C:\\Users\\Alexey\\anaconda3\\envs\\deep-learning-mptu\\lib\\runpy.py\", line 86, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"C:\\Users\\Alexey\\anaconda3\\envs\\deep-learning-mptu\\lib\\site-packages\\wget.py\", line 568, in <module>\n",
            "    filename = download(args[0], out=options.output)\n",
            "  File \"C:\\Users\\Alexey\\anaconda3\\envs\\deep-learning-mptu\\lib\\site-packages\\wget.py\", line 533, in download\n",
            "    filename = filename_fix_existing(filename)\n",
            "  File \"C:\\Users\\Alexey\\anaconda3\\envs\\deep-learning-mptu\\lib\\site-packages\\wget.py\", line 269, in filename_fix_existing\n",
            "    name, ext = filename.rsplit('.', 1)\n",
            "ValueError: not enough values to unpack (expected 2, got 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_LGFlYkxiZnN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e1ee821-1f30-4c48-b9d6-fc1dc8d76855"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Please consider pre-processing before quantization. See https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n",
            "C:\\Users\\Alexey\\AppData\\Local\\Temp\\ipykernel_4724\\2249352223.py:8: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n",
            "  image = image.resize((width, height), Image.ANTIALIAS)\n",
            "WARNING:root:Please consider pre-processing before quantization. See https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ONNX full precision model size (MB): 13.344060897827148\n",
            "ONNX quantized model size (MB): 3.4843339920043945\n"
          ]
        }
      ],
      "source": [
        "# change it to your real calibration data set\n",
        "calibration_data_folder = \"calibration_imagenet\"\n",
        "dr = MobilenetDataReader(calibration_data_folder)\n",
        "\n",
        "quantize_static('mobilenet_v2_float.onnx',\n",
        "                'mobilenet_v2_uint8.onnx',\n",
        "                dr)\n",
        "\n",
        "print('ONNX full precision model size (MB):', os.path.getsize(\"mobilenet_v2_float.onnx\")/(1024*1024))\n",
        "print('ONNX quantized model size (MB):', os.path.getsize(\"mobilenet_v2_uint8.onnx\")/(1024*1024))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FokmQGx7iZnO"
      },
      "source": [
        "## 2.3 Run the model with OnnxRuntime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l6v6gjLIiZnO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfc1f51a-da1b-4daa-da76-e34ed37fe151"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tabby 0.71912986\n",
            "tiger cat 0.1969283\n",
            "Egyptian cat 0.07271367\n",
            "lynx 0.0033133852\n",
            "tiger 0.002999183\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\Alexey\\AppData\\Local\\Temp\\ipykernel_4724\\2249352223.py:8: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n",
            "  image = image.resize((width, height), Image.ANTIALIAS)\n"
          ]
        }
      ],
      "source": [
        "session_quant = onnxruntime.InferenceSession(\"mobilenet_v2_uint8.onnx\")\n",
        "run_sample(session_quant, 'cat.jpg', categories)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}